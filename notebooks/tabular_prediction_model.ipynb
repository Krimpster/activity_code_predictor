{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Imports and Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "root_dir_path = os.getcwd()\n",
    "root_dir_path = os.path.abspath(os.path.split(root_dir_path)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.common import space\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_code</th>\n",
       "      <th>account_code</th>\n",
       "      <th>counterpart_code</th>\n",
       "      <th>business_code</th>\n",
       "      <th>activity_code</th>\n",
       "      <th>supplier</th>\n",
       "      <th>KPB AKTIVITETSKOD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>680000</td>\n",
       "      <td>30102</td>\n",
       "      <td>EP0000</td>\n",
       "      <td>1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A_ÖVR_504_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>680000</td>\n",
       "      <td>49000</td>\n",
       "      <td>AÖKA00</td>\n",
       "      <td>1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A_ÖVR_504_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>680000</td>\n",
       "      <td>50100</td>\n",
       "      <td>EP0000</td>\n",
       "      <td>1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A_ÖVR_504_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>680000</td>\n",
       "      <td>50460</td>\n",
       "      <td>EP0000</td>\n",
       "      <td>1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A_ÖVR_504_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>680000</td>\n",
       "      <td>55211</td>\n",
       "      <td>EP0000</td>\n",
       "      <td>1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A_ÖVR_504_0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   clean_code  account_code counterpart_code  business_code  activity_code  \\\n",
       "0      680000         30102           EP0000           1000            NaN   \n",
       "1      680000         49000           AÖKA00           1000            NaN   \n",
       "2      680000         50100           EP0000           1000            NaN   \n",
       "3      680000         50460           EP0000           1000            NaN   \n",
       "4      680000         55211           EP0000           1000            NaN   \n",
       "\n",
       "  supplier KPB AKTIVITETSKOD  \n",
       "0      NaN       A_ÖVR_504_0  \n",
       "1      NaN       A_ÖVR_504_0  \n",
       "2      NaN       A_ÖVR_504_0  \n",
       "3      NaN       A_ÖVR_504_0  \n",
       "4      NaN       A_ÖVR_504_0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_path = rf'{root_dir_path}/models/'\n",
    "\n",
    "train_df = pd.read_csv(rf'{root_dir_path}/data/train_val_csv.csv', delimiter= '|', encoding='utf-8', index_col=0)\n",
    "\n",
    "train_data = TabularDataset(train_df)\n",
    "label = \"KPB AKTIVITETSKOD\"\n",
    "\n",
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparam Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_tune = True\n",
    "\n",
    "nn_options = {\n",
    "    'num_epochs': 10,\n",
    "    'learning_rate': space.Real(1e-4, 1e-2, default=5e-4, log=True),\n",
    "    'layers': space.Categorical([100],[1000],[200,100],[300,200,100]),\n",
    "    'dropout_prob': space.Real(0.0, 0.5, default=0.1),\n",
    "}\n",
    "\n",
    "gbm_options = {\n",
    "    'num_boost_round': 100,\n",
    "    'num_leaves': space.Int(lower=26, upper=66, default=36),\n",
    "}\n",
    "\n",
    "hyperparameters = {\n",
    "                   'GBM': gbm_options,\n",
    "                   'NN_TORCH': nn_options,\n",
    "                  }\n",
    "\n",
    "time_limits = 2*60\n",
    "num_trials = 5 \n",
    "search_strategy = 'auto'\n",
    "\n",
    "hyperparameter_tune_kwargs = {  # HPO is not performed unless hyperparameter_tune_kwargs is specified\n",
    "    'num_trials': num_trials,\n",
    "    'scheduler' : 'local',\n",
    "    'searcher': search_strategy,\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"c:\\Users\\vicke\\Wizdo\\invoice_code_classifier/models/\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.1\n",
      "Python Version:     3.9.20\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          4\n",
      "Memory Avail:       4.21 GB / 15.88 GB (26.5%)\n",
      "Disk Space Avail:   63.31 GB / 236.39 GB (26.8%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
      "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
      "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
      "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
      "Warning: hyperparameter tuning is currently experimental and may cause the process to hang.\n",
      "Beginning AutoGluon training ... Time limit = 120s\n",
      "AutoGluon will save models to \"c:\\Users\\vicke\\Wizdo\\invoice_code_classifier/models/\"\n",
      "Train Data Rows:    46136\n",
      "Train Data Columns: 6\n",
      "Label Column:       KPB AKTIVITETSKOD\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == object).\n",
      "\tFirst 10 (of 357) unique label values:  ['A_ÖVR_504_0', 'A_ADM_600_1', 'A_ÖPP_405_0', 'A_ADM_600_5', 'A_ÖPP_404_0', 'A_LSS_130_0', 'A_ADM_601_1', 'A_LSS_121_0', 'A_LSS_119_0', 'A_ORB_209_1']\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Warning: Some classes in the training set have fewer than 10 examples. AutoGluon will only keep 328 out of 357 classes for training and will not try to predict the rare classes. To keep more classes, increase the number of datapoints from these rare classes in the training data or reduce label_count_threshold.\n",
      "Fraction of data from classes with at least 10 examples that will be kept for training models: 0.9969438182764002\n",
      "Train Data Class Count: 328\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    4316.53 MB\n",
      "\tTrain Data (Original)  Memory Usage: 7.12 MB (0.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  : 1 | ['activity_code']\n",
      "\t\t('int', [])    : 3 | ['clean_code', 'account_code', 'business_code']\n",
      "\t\t('object', []) : 2 | ['counterpart_code', 'supplier']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 2 | ['counterpart_code', 'supplier']\n",
      "\t\t('float', [])    : 1 | ['activity_code']\n",
      "\t\t('int', [])      : 3 | ['clean_code', 'account_code', 'business_code']\n",
      "\t0.4s = Fit runtime\n",
      "\t6 features in original data used to generate 6 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 1.54 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.55s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.10837523842552453, Train Rows: 41010, Val Rows: 4985\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'GBM': {'num_boost_round': 100, 'num_leaves': Int: lower=26, upper=66},\n",
      "\t'NN_TORCH': {'num_epochs': 10, 'learning_rate': Real: lower=0.0001, upper=0.01, 'layers': Categorical[[100], [1000], [200, 100], [300, 200, 100]], 'dropout_prob': Real: lower=0.0, upper=0.5},\n",
      "}\n",
      "Fitting 2 L1 models ...\n",
      "Hyperparameter tuning model: LightGBM ... Tuning model for up to 53.75s of the 119.45s of remaining time.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaf6c730b04645a3a760332549de647e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 26. Best iteration is:\n",
      "\t[1]\tvalid_set's multi_error: 0.251153\n",
      "\tStopping HPO to satisfy time limit...\n",
      "Fitted model: LightGBM\\T1 ...\n",
      "\t0.749\t = Validation score   (accuracy)\n",
      "\t49.48s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Hyperparameter tuning model: NeuralNetTorch ... Tuning model for up to 53.75s of the 69.66s of remaining time.\n",
      "Warning: Exception caused NeuralNetTorch to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\vicke\\anaconda3\\envs\\Wizdo_2\\lib\\site-packages\\ray\\tune\\search\\hyperopt\\hyperopt_search.py\", line 254, in _lookup\n",
      "    idx = categories.index(config_dict[key])\n",
      "ValueError: [100] is not in list\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\vicke\\anaconda3\\envs\\Wizdo_2\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2236, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\vicke\\anaconda3\\envs\\Wizdo_2\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1501, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(hpo_executor=hpo_executor, **kwargs)\n",
      "  File \"c:\\Users\\vicke\\anaconda3\\envs\\Wizdo_2\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1564, in _hyperparameter_tune\n",
      "    hpo_executor.execute(\n",
      "  File \"c:\\Users\\vicke\\anaconda3\\envs\\Wizdo_2\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 408, in execute\n",
      "    analysis = run(\n",
      "  File \"c:\\Users\\vicke\\anaconda3\\envs\\Wizdo_2\\lib\\site-packages\\autogluon\\core\\hpo\\ray_hpo.py\", line 281, in run\n",
      "    results = tuner.fit()\n",
      "  File \"c:\\Users\\vicke\\anaconda3\\envs\\Wizdo_2\\lib\\site-packages\\ray\\tune\\tuner.py\", line 379, in fit\n",
      "    return self._local_tuner.fit()\n",
      "  File \"c:\\Users\\vicke\\anaconda3\\envs\\Wizdo_2\\lib\\site-packages\\ray\\tune\\impl\\tuner_internal.py\", line 477, in fit\n",
      "    analysis = self._fit_internal(trainable, param_space)\n",
      "  File \"c:\\Users\\vicke\\anaconda3\\envs\\Wizdo_2\\lib\\site-packages\\ray\\tune\\impl\\tuner_internal.py\", line 596, in _fit_internal\n",
      "    analysis = run(\n",
      "  File \"c:\\Users\\vicke\\anaconda3\\envs\\Wizdo_2\\lib\\site-packages\\ray\\tune\\tune.py\", line 841, in run\n",
      "    if config and not searcher_set_search_props(\n",
      "  File \"c:\\Users\\vicke\\anaconda3\\envs\\Wizdo_2\\lib\\site-packages\\ray\\tune\\search\\util.py\", line 19, in _set_search_properties_backwards_compatible\n",
      "    return set_search_properties_func(metric, mode, config, **spec)\n",
      "  File \"c:\\Users\\vicke\\anaconda3\\envs\\Wizdo_2\\lib\\site-packages\\ray\\tune\\search\\search_generator.py\", line 63, in set_search_properties\n",
      "    return _set_search_properties_backwards_compatible(\n",
      "  File \"c:\\Users\\vicke\\anaconda3\\envs\\Wizdo_2\\lib\\site-packages\\ray\\tune\\search\\util.py\", line 19, in _set_search_properties_backwards_compatible\n",
      "    return set_search_properties_func(metric, mode, config, **spec)\n",
      "  File \"c:\\Users\\vicke\\anaconda3\\envs\\Wizdo_2\\lib\\site-packages\\ray\\tune\\search\\hyperopt\\hyperopt_search.py\", line 294, in set_search_properties\n",
      "    self._setup_hyperopt()\n",
      "  File \"c:\\Users\\vicke\\anaconda3\\envs\\Wizdo_2\\lib\\site-packages\\ray\\tune\\search\\hyperopt\\hyperopt_search.py\", line 224, in _setup_hyperopt\n",
      "    self._convert_categories_to_indices(config)\n",
      "  File \"c:\\Users\\vicke\\anaconda3\\envs\\Wizdo_2\\lib\\site-packages\\ray\\tune\\search\\hyperopt\\hyperopt_search.py\", line 277, in _convert_categories_to_indices\n",
      "    _lookup(config, self._space, k)\n",
      "  File \"c:\\Users\\vicke\\anaconda3\\envs\\Wizdo_2\\lib\\site-packages\\ray\\tune\\search\\hyperopt\\hyperopt_search.py\", line 273, in _lookup\n",
      "    raise ValueError(msg) from exc\n",
      "ValueError: Did not find category with value `[100]` in hyperopt parameter `layers`. Please make sure the specified category is valid.\n",
      "Did not find category with value `[100]` in hyperopt parameter `layers`. Please make sure the specified category is valid.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 119.45s of the 69.34s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM\\T1': 1.0}\n",
      "\t0.749\t = Validation score   (accuracy)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 50.9s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 56621.9 rows/s (4985 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Users\\vicke\\Wizdo\\invoice_code_classifier/models/\")\n"
     ]
    }
   ],
   "source": [
    "column_predictor = TabularPredictor(label=label, path=model_save_path).fit(train_data,\n",
    "    time_limit=time_limits,\n",
    "    hyperparameters=hyperparameters,\n",
    "    hyperparameter_tune_kwargs=hyperparameter_tune_kwargs,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"c:\\Users\\vicke\\Wizdo\\invoice_code_classifier/models/\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.1\n",
      "Python Version:     3.9.20\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          4\n",
      "Memory Avail:       4.47 GB / 15.88 GB (28.1%)\n",
      "Disk Space Avail:   63.15 GB / 236.39 GB (26.7%)\n",
      "===================================================\n",
      "Presets specified: ['high_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "Note: `save_bag_folds=False`! This will greatly reduce peak disk usage during fit (by ~8x), but runs the risk of an out-of-memory error during model refit if memory is small relative to the data size.\n",
      "\tYou can avoid this risk by setting `save_bag_folds=True`.\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 900s of the 3600s of remaining time (25%).\n",
      "\t\tContext path: \"c:\\Users\\vicke\\Wizdo\\invoice_code_classifier/models/ds_sub_fit\\sub_fit_ho\"\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                         model  score_holdout  score_val eval_metric  pred_time_test  pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0   KNeighborsDist_BAG_L1_FULL       0.777019   0.783166    accuracy        0.091093       0.366535    0.716551                 0.091093                0.366535           0.716551            1       True          2\n",
      "1     WeightedEnsemble_L3_FULL       0.776434   0.786643    accuracy        6.103925            NaN  153.541904                 0.077907                     NaN          34.984287            3       True          6\n",
      "2     WeightedEnsemble_L2_FULL       0.776434   0.786643    accuracy        6.114306            NaN  153.251773                 0.088288                     NaN          34.694156            2       True          5\n",
      "3   KNeighborsUnif_BAG_L1_FULL       0.666016   0.660097    accuracy        0.075676       0.393893    0.545151                 0.075676                0.393893           0.545151            1       True          1\n",
      "4       LightGBMXT_BAG_L1_FULL       0.332033   0.343208    accuracy        5.434951            NaN   26.454160                 5.434951                     NaN          26.454160            1       True          4\n",
      "5  NeuralNetFastAI_BAG_L1_FULL       0.272337   0.275663    accuracy        0.499974            NaN   91.386906                 0.499974                     NaN          91.386906            1       True          3\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t1199s\t = DyStack   runtime |\t2401s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 2401s\n",
      "AutoGluon will save models to \"c:\\Users\\vicke\\Wizdo\\invoice_code_classifier/models/\"\n",
      "Train Data Rows:    46136\n",
      "Train Data Columns: 6\n",
      "Label Column:       KPB AKTIVITETSKOD\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Warning: Some classes in the training set have fewer than 10 examples. AutoGluon will only keep 328 out of 357 classes for training and will not try to predict the rare classes. To keep more classes, increase the number of datapoints from these rare classes in the training data or reduce label_count_threshold.\n",
      "Fraction of data from classes with at least 10 examples that will be kept for training models: 0.9969438182764002\n",
      "Train Data Class Count: 328\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    5075.01 MB\n",
      "\tTrain Data (Original)  Memory Usage: 7.47 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  : 1 | ['activity_code']\n",
      "\t\t('int', [])    : 3 | ['clean_code', 'account_code', 'business_code']\n",
      "\t\t('object', []) : 2 | ['counterpart_code', 'supplier']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 2 | ['counterpart_code', 'supplier']\n",
      "\t\t('float', [])    : 1 | ['activity_code']\n",
      "\t\t('int', [])      : 3 | ['clean_code', 'account_code', 'business_code']\n",
      "\t0.5s = Fit runtime\n",
      "\t6 features in original data used to generate 6 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 1.54 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.67s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 110 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 1600.08s of the 2400.62s of remaining time.\n",
      "\t0.6726\t = Validation score   (accuracy)\n",
      "\t0.55s\t = Training   runtime\n",
      "\t0.42s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 1598.39s of the 2398.93s of remaining time.\n",
      "\t0.793\t = Validation score   (accuracy)\n",
      "\t0.6s\t = Training   runtime\n",
      "\t0.47s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 1596.85s of the 2397.4s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.42%)\n",
      "\t0.3085\t = Validation score   (accuracy)\n",
      "\t1212.0s\t = Training   runtime\n",
      "\t6.24s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 367.11s of the 1167.65s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=2.92%)\n",
      "\t0.4412\t = Validation score   (accuracy)\n",
      "\t410.66s\t = Training   runtime\n",
      "\t399.21s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 686.73s of remaining time.\n",
      "\tEnsemble Weights: {'KNeighborsDist_BAG_L1': 0.389, 'LightGBMXT_BAG_L1': 0.333, 'NeuralNetFastAI_BAG_L1': 0.278}\n",
      "\t0.799\t = Validation score   (accuracy)\n",
      "\t38.19s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting 108 L2 models ...\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 647.31s of the 644.96s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 1 folds in parallel instead (Estimated 42.41% memory usage per fold, 42.41%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=2, gpus=0, memory=42.41%)\n",
      "\t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\t0.8305\t = Validation score   (accuracy)\n",
      "\t594.49s\t = Training   runtime\n",
      "\t9.98s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 36.33s of the 33.69s of remaining time.\n",
      "\tWarning: Not enough memory to safely train model. Estimated to require 21.666 GB out of 6.716 GB available memory (322.620%)... (90.000% of avail memory is the max safe size)\n",
      "\tTo force training the model, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=3.63 to avoid the error)\n",
      "\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n",
      "\tNot enough memory to train LightGBMXT_BAG_L2... Skipping this model.\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 30.17s of the 27.76s of remaining time.\n",
      "\tWarning: Not enough memory to safely train model. Estimated to require 21.666 GB out of 6.540 GB available memory (331.292%)... (90.000% of avail memory is the max safe size)\n",
      "\tTo force training the model, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=3.73 to avoid the error)\n",
      "\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n",
      "\tNot enough memory to train LightGBM_BAG_L2... Skipping this model.\n",
      "Fitting model: RandomForestGini_BAG_L2 ... Training model for up to 24.99s of the 22.61s of remaining time.\n",
      "\tWarning: Not enough memory to safely train model. Estimated to require 10.058 GB out of 6.540 GB available memory (153.794%)... (50.000% of avail memory is the max safe size)\n",
      "\tTo force training the model, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=3.13 to avoid the error)\n",
      "\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n",
      "\tNot enough memory to train RandomForestGini_BAG_L2... Skipping this model.\n",
      "Fitting model: RandomForestEntr_BAG_L2 ... Training model for up to 20.03s of the 17.73s of remaining time.\n",
      "\tWarning: Not enough memory to safely train model. Estimated to require 10.058 GB out of 6.551 GB available memory (153.519%)... (50.000% of avail memory is the max safe size)\n",
      "\tTo force training the model, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=3.12 to avoid the error)\n",
      "\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n",
      "\tNot enough memory to train RandomForestEntr_BAG_L2... Skipping this model.\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 15.04s of the 12.73s of remaining time.\n",
      "\tWarning: Not enough memory to safely train model. Estimated to require 21.930 GB out of 6.510 GB available memory (336.872%)... (100.000% of avail memory is the max safe size)\n",
      "\tTo force training the model, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=3.42 to avoid the error)\n",
      "\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n",
      "\tNot enough memory to train CatBoost_BAG_L2... Skipping this model.\n",
      "Fitting model: ExtraTreesGini_BAG_L2 ... Training model for up to 10.02s of the 7.69s of remaining time.\n",
      "\tWarning: Not enough memory to safely train model. Estimated to require 10.058 GB out of 6.511 GB available memory (154.482%)... (50.000% of avail memory is the max safe size)\n",
      "\tTo force training the model, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=3.14 to avoid the error)\n",
      "\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n",
      "\tNot enough memory to train ExtraTreesGini_BAG_L2... Skipping this model.\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the -56.72s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI_BAG_L2': 0.522, 'KNeighborsDist_BAG_L1': 0.174, 'NeuralNetFastAI_BAG_L1': 0.174, 'LightGBMXT_BAG_L1': 0.13}\n",
      "\t0.8324\t = Validation score   (accuracy)\n",
      "\t43.41s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 2503.14s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 13.8 rows/s (5750 batch size)\n",
      "Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
      "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
      "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
      "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
      "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
      "Fitting model: KNeighborsUnif_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\t0.55s\t = Training   runtime\n",
      "\t0.42s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\t0.6s\t = Training   runtime\n",
      "\t0.47s\t = Validation runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: NeuralNetFastAI_BAG_L1_FULL ...\n",
      "\tStopping at the best epoch learned earlier - 20.\n",
      "\t240.08s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: LightGBMXT_BAG_L1_FULL ...\n",
      "\t73.2s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'KNeighborsDist_BAG_L1': 0.389, 'LightGBMXT_BAG_L1': 0.333, 'NeuralNetFastAI_BAG_L1': 0.278}\n",
      "\t38.19s\t = Training   runtime\n",
      "Fitting 1 L2 models ...\n",
      "Fitting model: NeuralNetFastAI_BAG_L2_FULL ...\n",
      "\tStopping at the best epoch learned earlier - 1.\n",
      "\t56.87s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'NeuralNetFastAI_BAG_L2': 0.522, 'KNeighborsDist_BAG_L1': 0.174, 'NeuralNetFastAI_BAG_L1': 0.174, 'LightGBMXT_BAG_L1': 0.13}\n",
      "\t43.41s\t = Training   runtime\n",
      "Updated best model to \"WeightedEnsemble_L3_FULL\" (Previously \"WeightedEnsemble_L3\"). AutoGluon will default to using \"WeightedEnsemble_L3_FULL\" for predict() and predict_proba().\n",
      "Refit complete, total runtime = 381.17s ... Best model: \"WeightedEnsemble_L3_FULL\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Users\\vicke\\Wizdo\\invoice_code_classifier/models/\")\n"
     ]
    }
   ],
   "source": [
    "column_predictor = TabularPredictor(label=label, path=model_save_path).fit(train_data, presets='high_quality')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Testing and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = TabularDataset(train_df)\n",
    "val_data = val_data.sample(n=30000, random_state = 42)\n",
    "val_data.drop(columns=['KPB AKTIVITETSKOD'])\n",
    "\n",
    "val_predictor = TabularPredictor.load(model_save_path)\n",
    "\n",
    "y_pred = val_predictor.predict(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_test</th>\n",
       "      <th>score_val</th>\n",
       "      <th>eval_metric</th>\n",
       "      <th>pred_time_test</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>pred_time_test_marginal</th>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>stack_level</th>\n",
       "      <th>can_infer</th>\n",
       "      <th>fit_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WeightedEnsemble_L2_FULL</td>\n",
       "      <td>0.996267</td>\n",
       "      <td>NaN</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>90.713551</td>\n",
       "      <td>NaN</td>\n",
       "      <td>352.074943</td>\n",
       "      <td>0.414694</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.190448</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNeighborsDist_BAG_L1_FULL</td>\n",
       "      <td>0.994333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.268339</td>\n",
       "      <td>0.466812</td>\n",
       "      <td>0.602914</td>\n",
       "      <td>0.268339</td>\n",
       "      <td>0.466812</td>\n",
       "      <td>0.602914</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNeighborsDist_BAG_L1</td>\n",
       "      <td>0.994333</td>\n",
       "      <td>0.793043</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.280466</td>\n",
       "      <td>0.466812</td>\n",
       "      <td>0.602914</td>\n",
       "      <td>0.280466</td>\n",
       "      <td>0.466812</td>\n",
       "      <td>0.602914</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WeightedEnsemble_L3_FULL</td>\n",
       "      <td>0.984833</td>\n",
       "      <td>NaN</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>98.127441</td>\n",
       "      <td>NaN</td>\n",
       "      <td>414.719679</td>\n",
       "      <td>0.489663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43.414313</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NeuralNetFastAI_BAG_L2_FULL</td>\n",
       "      <td>0.956933</td>\n",
       "      <td>NaN</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>97.637779</td>\n",
       "      <td>NaN</td>\n",
       "      <td>371.305366</td>\n",
       "      <td>7.007469</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56.866500</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>KNeighborsUnif_BAG_L1_FULL</td>\n",
       "      <td>0.803733</td>\n",
       "      <td>NaN</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.331453</td>\n",
       "      <td>0.416510</td>\n",
       "      <td>0.554371</td>\n",
       "      <td>0.331453</td>\n",
       "      <td>0.416510</td>\n",
       "      <td>0.554371</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>KNeighborsUnif_BAG_L1</td>\n",
       "      <td>0.803733</td>\n",
       "      <td>0.672595</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.561521</td>\n",
       "      <td>0.416510</td>\n",
       "      <td>0.554371</td>\n",
       "      <td>0.561521</td>\n",
       "      <td>0.416510</td>\n",
       "      <td>0.554371</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LightGBMXT_BAG_L1_FULL</td>\n",
       "      <td>0.550167</td>\n",
       "      <td>NaN</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>87.492453</td>\n",
       "      <td>NaN</td>\n",
       "      <td>73.204340</td>\n",
       "      <td>87.492453</td>\n",
       "      <td>NaN</td>\n",
       "      <td>73.204340</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NeuralNetFastAI_BAG_L1_FULL</td>\n",
       "      <td>0.349200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>2.538066</td>\n",
       "      <td>NaN</td>\n",
       "      <td>240.077241</td>\n",
       "      <td>2.538066</td>\n",
       "      <td>NaN</td>\n",
       "      <td>240.077241</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>WeightedEnsemble_L3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.832351</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>416.426901</td>\n",
       "      <td>2261.715888</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.114512</td>\n",
       "      <td>43.414313</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NeuralNetFastAI_BAG_L2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.830525</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>416.312389</td>\n",
       "      <td>2218.301575</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.982069</td>\n",
       "      <td>594.487520</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>WeightedEnsemble_L2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.799022</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>406.029145</td>\n",
       "      <td>1661.450132</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.115334</td>\n",
       "      <td>38.190448</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LightGBMXT_BAG_L1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.441200</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>399.207533</td>\n",
       "      <td>410.656663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>399.207533</td>\n",
       "      <td>410.656663</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NeuralNetFastAI_BAG_L1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.308534</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.239466</td>\n",
       "      <td>1212.000107</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.239466</td>\n",
       "      <td>1212.000107</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          model  score_test  score_val eval_metric  \\\n",
       "0      WeightedEnsemble_L2_FULL    0.996267        NaN    accuracy   \n",
       "1    KNeighborsDist_BAG_L1_FULL    0.994333        NaN    accuracy   \n",
       "2         KNeighborsDist_BAG_L1    0.994333   0.793043    accuracy   \n",
       "3      WeightedEnsemble_L3_FULL    0.984833        NaN    accuracy   \n",
       "4   NeuralNetFastAI_BAG_L2_FULL    0.956933        NaN    accuracy   \n",
       "5    KNeighborsUnif_BAG_L1_FULL    0.803733        NaN    accuracy   \n",
       "6         KNeighborsUnif_BAG_L1    0.803733   0.672595    accuracy   \n",
       "7        LightGBMXT_BAG_L1_FULL    0.550167        NaN    accuracy   \n",
       "8   NeuralNetFastAI_BAG_L1_FULL    0.349200        NaN    accuracy   \n",
       "9           WeightedEnsemble_L3         NaN   0.832351    accuracy   \n",
       "10       NeuralNetFastAI_BAG_L2         NaN   0.830525    accuracy   \n",
       "11          WeightedEnsemble_L2         NaN   0.799022    accuracy   \n",
       "12            LightGBMXT_BAG_L1         NaN   0.441200    accuracy   \n",
       "13       NeuralNetFastAI_BAG_L1         NaN   0.308534    accuracy   \n",
       "\n",
       "    pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  \\\n",
       "0        90.713551            NaN   352.074943                 0.414694   \n",
       "1         0.268339       0.466812     0.602914                 0.268339   \n",
       "2         0.280466       0.466812     0.602914                 0.280466   \n",
       "3        98.127441            NaN   414.719679                 0.489663   \n",
       "4        97.637779            NaN   371.305366                 7.007469   \n",
       "5         0.331453       0.416510     0.554371                 0.331453   \n",
       "6         0.561521       0.416510     0.554371                 0.561521   \n",
       "7        87.492453            NaN    73.204340                87.492453   \n",
       "8         2.538066            NaN   240.077241                 2.538066   \n",
       "9              NaN     416.426901  2261.715888                      NaN   \n",
       "10             NaN     416.312389  2218.301575                      NaN   \n",
       "11             NaN     406.029145  1661.450132                      NaN   \n",
       "12             NaN     399.207533   410.656663                      NaN   \n",
       "13             NaN       6.239466  1212.000107                      NaN   \n",
       "\n",
       "    pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
       "0                      NaN          38.190448            2       True   \n",
       "1                 0.466812           0.602914            1       True   \n",
       "2                 0.466812           0.602914            1       True   \n",
       "3                      NaN          43.414313            3       True   \n",
       "4                      NaN          56.866500            2       True   \n",
       "5                 0.416510           0.554371            1       True   \n",
       "6                 0.416510           0.554371            1       True   \n",
       "7                      NaN          73.204340            1       True   \n",
       "8                      NaN         240.077241            1       True   \n",
       "9                 0.114512          43.414313            3      False   \n",
       "10                9.982069         594.487520            2      False   \n",
       "11                0.115334          38.190448            2      False   \n",
       "12              399.207533         410.656663            1      False   \n",
       "13                6.239466        1212.000107            1      False   \n",
       "\n",
       "    fit_order  \n",
       "0          12  \n",
       "1           9  \n",
       "2           2  \n",
       "3          14  \n",
       "4          13  \n",
       "5           8  \n",
       "6           1  \n",
       "7          11  \n",
       "8          10  \n",
       "9           7  \n",
       "10          6  \n",
       "11          5  \n",
       "12          4  \n",
       "13          3  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_predictor.leaderboard(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Wizdo_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
